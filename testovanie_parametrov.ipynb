{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5502407636528575035\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 890245541389249908\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4925095936\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12976571041280552539\n",
      "physical_device_desc: \"device: 0, name: Quadro P2000, pci bus id: 0000:02:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6166074350547664903\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"GPU\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (20, 10)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "cfg = tf.compat.v1.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=cfg)\n",
    "K.set_session(sess)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load( open( \"/home/kriza/programing/clanok/data/picklnute/30min.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
    "  data = []\n",
    "  labels = []\n",
    "\n",
    "  start_index = start_index + history_size\n",
    "  if end_index is None:\n",
    "    end_index = len(dataset) - target_size\n",
    "\n",
    "  for i in range(start_index, end_index):\n",
    "    indices = range(i-history_size, i)\n",
    "    # Reshape data from (history_size,) to (history_size, 1)\n",
    "    data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "    labels.append(dataset[i+target_size])\n",
    "  return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT_BEGIN = 24\n",
    "TRAIN_SPLIT_END = 15551 - TRAIN_SPLIT_BEGIN\n",
    "VALID_SPLIT_BEGIN = 15601 - TRAIN_SPLIT_BEGIN\n",
    "VALID_SPLIT_END = 17463 -TRAIN_SPLIT_BEGIN\n",
    "tf.random.set_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data = df['30m-item61']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data = uni_data.values\n",
    "uni_data = uni_data[TRAIN_SPLIT_BEGIN:VALID_SPLIT_END + TRAIN_SPLIT_BEGIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_train_mean = uni_data[:TRAIN_SPLIT_END].mean()\n",
    "uni_train_std = uni_data[:TRAIN_SPLIT_END].std()\n",
    "uni_data = (uni_data-uni_train_mean)/uni_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/kriza/predikcie1krok\" target=\"_blank\">https://app.wandb.ai/kriza/predikcie1krok</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/kriza/predikcie1krok/runs/29so0nd2\" target=\"_blank\">https://app.wandb.ai/kriza/predikcie1krok/runs/29so0nd2</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    }
   ],
   "source": [
    "UNIT_COUNT = range(10, 30, 10)\n",
    "HISTORY = range(20, 30, 10)\n",
    "BATCH_SIZES = [64, 128, 256, 512]\n",
    "\n",
    "\n",
    "univariate_past_history = 0\n",
    "BATCH_SIZE = 0\n",
    "UNITS = 0\n",
    "saved_models = \"\"\n",
    "\n",
    "for i in BATCH_SIZES:\n",
    "    for j in HISTORY:\n",
    "        for k in UNIT_COUNT:\n",
    "            univariate_past_history = j\n",
    "            BATCH_SIZE = i\n",
    "            UNITS = k\n",
    "            saved_models = \"saved_models/\" + \"hist=\" + str(univariate_past_history) + \"batch=\" + str(BATCH_SIZE) + \"units=\" + str(UNITS) + \"/model\"\n",
    "            wandb.init(project=\"predikcie1krok\", name = \"hist=\" + str(univariate_past_history) + \"batch=\" + str(BATCH_SIZE) + \"units=\" + str(UNITS))\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu cyklus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "univariate_future_target = 0\n",
    "\n",
    "x_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT_END,\n",
    "                                           univariate_past_history,\n",
    "                                           univariate_future_target)\n",
    "x_val_uni, y_val_uni = univariate_data(uni_data, VALID_SPLIT_BEGIN, None,\n",
    "                                       univariate_past_history,\n",
    "                                       univariate_future_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
    "val_univariate = val_univariate.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(UNITS, input_shape=x_train_uni.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "simple_lstm_model.compile(optimizer='adam', loss='mae', metrics='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "  1/243 [..............................] - ETA: 0s - loss: 1.1757 - mae: 1.1757WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.260146). Check your callbacks.\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.5914 - mae: 0.5914 - val_loss: 2.7060 - val_mae: 2.7060\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 1s 2ms/step - loss: 0.2360 - mae: 0.2360 - val_loss: 2.0557 - val_mae: 2.0557\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 1s 2ms/step - loss: 0.1915 - mae: 0.1915 - val_loss: 1.8875 - val_mae: 1.8875\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 1s 2ms/step - loss: 0.1808 - mae: 0.1808 - val_loss: 1.7871 - val_mae: 1.7871\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 1s 2ms/step - loss: 0.1772 - mae: 0.1772 - val_loss: 1.7184 - val_mae: 1.7184\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 1s 2ms/step - loss: 0.1753 - mae: 0.1753 - val_loss: 1.6653 - val_mae: 1.6653\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 1s 2ms/step - loss: 0.1742 - mae: 0.1742 - val_loss: 1.6243 - val_mae: 1.6243\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1732 - mae: 0.1732 - val_loss: 1.5814 - val_mae: 1.5814\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 1s 2ms/step - loss: 0.1725 - mae: 0.1725 - val_loss: 1.5489 - val_mae: 1.5489\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 1s 2ms/step - loss: 0.1728 - mae: 0.1728 - val_loss: 1.5143 - val_mae: 1.5143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f31e6cd2780>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "ckpt = ModelCheckpoint(filepath=saved_models, monitor='val_loss', mode='min', save_best_only = True, save_weights_only=True, verbose = False)\n",
    "\n",
    "simple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n",
    "                    #   steps_per_epoch=int(x_train_uni.shape[0]/BATCH_SIZE),\n",
    "                      validation_data=val_univariate,\n",
    "                      callbacks=[ckpt, WandbCallback()]\n",
    "                    #   , validation_steps=50\n",
    "                      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.2-gpu",
   "language": "python",
   "name": "tf2.2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
